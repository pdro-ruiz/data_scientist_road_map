{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "502ac205-ba44-486b-aa75-5b6a0798f786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "# Cargar las stopwords en inglés, francés, alemán, español y ruso\n",
    "stop_words = set(stopwords.words('english')).union(\n",
    "    stopwords.words('french'),\n",
    "    stopwords.words('german'),\n",
    "    stopwords.words('spanish'),\n",
    "    stopwords.words('russian')\n",
    ")\n",
    "\n",
    "\n",
    "def remove_characters(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Recorre todos los archivos CSV en el directorio de entrada, realiza la sustitución en las columnas de cada archivo y guarda los archivos modificados en el directorio de salida. \n",
    "    También copia los archivos .json al directorio de salida.\n",
    "\n",
    "    Parámetros:\n",
    "        - input_dir: Directorio de entrada\n",
    "        - output_dir: Directorio de salida\n",
    "    \"\"\"\n",
    "    # Crea el directorio de salida si no existe\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Recorre todos los archivos en el directorio de entrada\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            # Ruta al archivo de entrada\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            # Ruta al archivo de salida\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            # Intenta leer los datos con pandas en formato 'utf-8'\n",
    "            try:\n",
    "                df = pd.read_csv(input_path, encoding='utf-8')\n",
    "            except UnicodeDecodeError:\n",
    "                # Si falla, intenta con 'ISO-8859-1'\n",
    "                df = pd.read_csv(input_path, encoding='ISO-8859-1')\n",
    "\n",
    "            # Realiza la sustitución en todas las columnas\n",
    "            df = df.replace('\\n', ' ', regex=True).replace('\\r', ' ', regex=True)\n",
    "\n",
    "            # Guarda los datos modificados en el archivo de salida\n",
    "            df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        elif filename.endswith('.json'):\n",
    "            # Ruta al archivo de entrada\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            # Ruta al archivo de salida\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            # Copia el archivo .json al directorio de salida\n",
    "            shutil.copy(input_path, output_path)\n",
    "\n",
    "\n",
    "def extract_keywords(kw_col):\n",
    "    # Eliminar todos los caracteres no alfabéticos y convertir a minúsculas\n",
    "    cleaned_text = re.sub(r'\\W+', ' ', kw_col).lower()\n",
    "    # Dividir el texto en palabras\n",
    "    words = re.findall(r'\\b\\w{4,}\\b', cleaned_text)\n",
    "    # Eliminar números de las palabras\n",
    "    words = [re.sub(r'\\d', '', word) for word in words]\n",
    "    return words\n",
    "\n",
    "\n",
    "def remove_stopwords(keywords):\n",
    "    # Eliminar las stopwords\n",
    "    words = [word for word in keywords if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "\n",
    "def load_data(csv_path, json_path, output_path):\n",
    "    \"\"\"\n",
    "    Loads CSV and JSON files, performs preprocessing and returns a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        - csv_path: Path to the CSV file.\n",
    "        - json_path: Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        - DataFrame with the loaded and preprocessed data.\n",
    "    \"\"\"    \n",
    "    # load CSV data with different encodings\n",
    "    data = pd.read_csv(csv_path, encoding='utf-8', quotechar='\"', quoting=1)\n",
    "    \n",
    "    # Asumiendo que 'data' es tu DataFrame\n",
    "    data = data[data['category_id'] != 29]\n",
    "    \n",
    "    # SXXXX\n",
    "    data['title'].replace('', np.nan, inplace=True)\n",
    "    data['tags'].replace('', np.nan, inplace=True)\n",
    "    data = data.dropna(subset=['title', 'tags'])\n",
    "    \n",
    "    # transform date\n",
    "    data['trending_date'] = pd.to_datetime(data['trending_date'], format='%y.%d.%m')\n",
    "    data['publish_time'] = pd.to_datetime(data['publish_time'])\n",
    "\n",
    "    # Ahora podemos extraer la hora y el día de la semana\n",
    "    data['publish_hour'] = data['publish_time'].dt.hour\n",
    "    data['publish_day_of_week'] = data['publish_time'].dt.dayofweek\n",
    "    \n",
    "    # Tratar columnas booleanas\n",
    "    boolean_col = ['comments_disabled', 'ratings_disabled', 'video_error_or_removed']\n",
    "    for col in boolean_col:\n",
    "        data[col] = data[col].astype(int)\n",
    "    \n",
    "    # Load JSON data\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            categories = json.load(f)['items']\n",
    "\n",
    "    # Crear un diccionario para mapear los IDs de las categorías a sus nombres.\n",
    "    category_dict = {int(cat['id']): cat['snippet']['title'] for cat in categories}\n",
    "\n",
    "    # Agregar una nueva columna con los nombres de las categorías de los videos\n",
    "    data['category_name'] = data['category_id'].map(category_dict)\n",
    "    \n",
    "    # Create a combined column for video_id and trending_date\n",
    "    data['id_date'] = data['video_id'] + data['trending_date'].astype(str)\n",
    "    \n",
    "    # Filter new data to exclude videos that are already in the old data\n",
    "    if os.path.exists(output_path):\n",
    "        old_data = pd.read_csv(output_path)\n",
    "        old_data['id_date'] = old_data['video_id'] + old_data['trending_date'].astype(str)\n",
    "        new_data = data[~data['id_date'].isin(old_data['id_date'])]\n",
    "        data = pd.concat([old_data, new_data])\n",
    "\n",
    "    # Aplicar las funciones a la columna 'title' y convertir las listas en tuplas\n",
    "    data['title'] = data['title'].astype(str)\n",
    "    data['title_kw'] = data['title'].apply(extract_keywords).apply(remove_stopwords)\n",
    "    data['title_kw'] = data['title_kw'].apply(tuple)\n",
    "\n",
    "    # Aplicar las funciones a la columna 'tags' y convertir las listas en tuplas\n",
    "    data['tags'] = data['tags'].astype(str)\n",
    "    data['tags_kw'] = data['tags'].apply(extract_keywords).apply(remove_stopwords)\n",
    "    data['tags_kw'] = data['tags_kw'].apply(tuple)\n",
    "\n",
    "    # Aplicar las funciones a la columna 'description' y convertir las listas en tuplas\n",
    "    data['description'] = data['description'].astype(str)\n",
    "    data['description_kw'] = data['description'].apply(extract_keywords).apply(remove_stopwords)\n",
    "    data['description_kw'] = data['description_kw'].apply(list)\n",
    "    \n",
    "    # XXXXX\n",
    "    data['title_length'] = data['title'].str.len()\n",
    "    data['description_length'] = data['description'].str.len()\n",
    "    data['tags_count'] = data['tags_kw'].apply(lambda x: len(x) if isinstance(x, tuple) else 0)\n",
    "    \n",
    "    # Logarithms\n",
    "    data['log_views'] = np.log1p(data['views'])\n",
    "    data['log_likes'] = np.log1p(data['likes'])\n",
    "    data['log_dislikes'] = np.log1p(data['dislikes'])\n",
    "    data['log_comment_count'] = np.log1p(data['comment_count'])\n",
    "    \n",
    "    \n",
    "    # New charatceristics with the abbreviation of the data country\n",
    "    data['region'] = os.path.basename(csv_path)[:2]\n",
    "    \n",
    "    # Create a combined column for video_id and trending_date\n",
    "    data['id_date'] = data['video_id'] + data['trending_date'].astype(str)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    1. Load, preprocess and save the data for each region in a separate file. \n",
    "    2. Saves all data in a single file.\n",
    "    \n",
    "    Parameters:\n",
    "            - input_dir: Data directory\n",
    "            - output_dir: Storage directory of the processed data.\n",
    "    \"\"\"\n",
    "    # Create a list to store all data\n",
    "    all_data = []\n",
    "    \n",
    "    # loop over all files in the data directory\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            # File upload\n",
    "            csv_path = os.path.join(input_dir, filename)\n",
    "            json_path = os.path.join(input_dir, filename[:2] + '_category_id.json')\n",
    "            output_path = os.path.join(output_dir, filename[:2] + '_processed.csv')\n",
    "            data = load_data(csv_path, json_path, output_path)\n",
    "    \n",
    "            # Filter new data to exclude videos that are already in the old data\n",
    "            if os.path.exists(output_path):\n",
    "                old_data = pd.read_csv(output_path)\n",
    "                old_data['id_date'] = old_data['video_id'] + old_data['trending_date'].astype(str)\n",
    "                new_data = data[~data['id_date'].isin(old_data['id_date'])]\n",
    "                data = pd.concat([old_data, new_data])\n",
    "            \n",
    "            # Add to list\n",
    "            all_data.append(data)\n",
    "            \n",
    "            # Saves the data in a separate file for each region.\n",
    "            data.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Concatenate all data and save in a single file\n",
    "    all_data = pd.concat(all_data)\n",
    "    all_data.to_csv(os.path.join(output_dir, 'ALL_data_processed.csv'), index=False)\n",
    "    \n",
    "    # Load the 'ALL_data_processed.csv' file\n",
    "    all_data_processed = pd.read_csv(os.path.join(output_dir, 'ALL_data_processed.csv'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Crear un objeto LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    le.fit(all_data_processed['region'])\n",
    "\n",
    "    # Transformar los datos en la columna 'region' de all_data_processed\n",
    "    all_data_processed['region_ncod'] = le.transform(all_data_processed['region'])\n",
    "    all_data_processed.to_csv(os.path.join(output_dir, 'ALL_data_processed.csv'), index=False)\n",
    "\n",
    "\n",
    "# Llamamos a la función para eliminar la columna 'comments' de los archivos CSV y copiar los archivos .json\n",
    "remove_characters('data', 'data/pre_processed')\n",
    "\n",
    "# We call the function\n",
    "preprocess_data('data/pre_processed', 'data/processed')\n",
    "\n",
    "# Eliminar la carpeta 'pre_processed'\n",
    "folder_path = 'data/pre_processed'\n",
    "if os.path.exists(folder_path):\n",
    "    shutil.rmtree(folder_path)\n",
    "    os.makedirs(folder_path)\n",
    "else:\n",
    "    print(\"La carpeta no existe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f7abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
